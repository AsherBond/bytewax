<main class="api__content">
<article class="api__article" id="content">
<header class="api__article-header">
<h1 class="api__article-title">Module <strong>bytewax.inputs</strong></h1>
</header>
<section class="api__article-intro" id="section-intro">
<p>Low-level input interfaces and input helpers.</p>
<p>If you want pre-built connectors for various external systems, see
<code><a title="bytewax.connectors" href="/apidocs/bytewax.connectors/index">bytewax.connectors</a></code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">&#34;&#34;&#34;Low-level input interfaces and input helpers.

If you want pre-built connectors for various external systems, see
`bytewax.connectors`.

&#34;&#34;&#34;

import asyncio
import queue
from abc import ABC, abstractmethod
from collections.abc import AsyncIterable
from dataclasses import dataclass
from datetime import datetime, timedelta
from itertools import islice
from typing import (
    Any,
    Callable,
    Generic,
    Iterable,
    Iterator,
    List,
    Optional,
    Type,
    TypeVar,
)

from .bytewax import AbortExecution  # type: ignore[import]

__all__ = [
    &#34;AbortExecution&#34;,
    &#34;DynamicSource&#34;,
    &#34;FixedPartitionedSource&#34;,
    &#34;SimplePollingSource&#34;,
    &#34;Source&#34;,
    &#34;StatefulSourcePartition&#34;,
    &#34;StatelessSourcePartition&#34;,
    &#34;batch&#34;,
    &#34;batch_async&#34;,
    &#34;batch_getter&#34;,
    &#34;batch_getter_ex&#34;,
]

X = TypeVar(&#34;X&#34;)
S = TypeVar(&#34;S&#34;)


class Source(ABC, Generic[X]):  # noqa: B024
    &#34;&#34;&#34;A location to read input items from.

    Base class for all input sources. Do not subclass this.

    If you want to implement a custom connector, instead subclass one
    of the specific source sub-types below in this module.

    &#34;&#34;&#34;

    pass


class StatefulSourcePartition(ABC, Generic[X, S]):
    &#34;&#34;&#34;Input partition that maintains state of its position.&#34;&#34;&#34;

    @abstractmethod
    def next_batch(self, sched: datetime) -&gt; Iterable[X]:
        &#34;&#34;&#34;Attempt to get the next batch of input items.

        This must participate in a kind of cooperative multi-tasking,
        never blocking but returning an empty list if there are no
        items to emit yet.

        Args:
            sched: The scheduled awake time.

        Returns:
            Items immediately ready. May be empty if no new items.

        Raises:
            StopIteration: When the source is complete.

        &#34;&#34;&#34;
        ...

    def next_awake(self) -&gt; Optional[datetime]:
        &#34;&#34;&#34;When to next attempt to get input items.

        `next_batch()` will not be called until the most recently returned
        time has past.

        This will be called upon initialization of the source and
        after `next_batch()`, but also possibly at other times. Multiple
        times are not stored; you must return the next awake time on
        every call, if any.

        If this returns `None`, `next_batch()` will be called
        immediately unless the previous batch had no items, in which
        case there is a 1 millisecond delay.

        Use this instead of `time.sleep` in `next_batch()`.

        Returns:
            Next awake time or `None` to indicate automatic behavior.

        &#34;&#34;&#34;
        return None

    @abstractmethod
    def snapshot(self) -&gt; S:
        &#34;&#34;&#34;Snapshot the position of the next read of this partition.

        This will be returned to you via the `resume_state` parameter
        of your input builder.

        Be careful of &#34;off by one&#34; errors in resume state. This should
        return a state that, when built into a partition, resumes reading
        _after the last read item item_, not the same item that
        `next()` last returned.

        This is guaranteed to never be called after `close()`.

        Returns:
            Resume state.

        &#34;&#34;&#34;
        ...

    def close(self) -&gt; None:
        &#34;&#34;&#34;Cleanup this partition when the dataflow completes.

        This is not guaranteed to be called. It will only be called
        when the dataflow finishes on finite input. It will not be
        called during an abrupt or abort shutdown.

        &#34;&#34;&#34;
        return


class FixedPartitionedSource(Source[X], Generic[X, S]):
    &#34;&#34;&#34;An input source with a fixed number of independent partitions.

    Will maintain the state of each source and re-build using it
    during resume. If the source supports seeking, this input can
    support exactly-once processing.

    Each partition must contain unique data. If you re-read the same data
    in multiple partitions, the dataflow will process these duplicate
    items.

    &#34;&#34;&#34;

    @abstractmethod
    def list_parts(self) -&gt; List[str]:
        &#34;&#34;&#34;List all local partitions this worker has access to.

        You do not need to list all partitions globally.

        Returns:
            Local partition keys.

        &#34;&#34;&#34;
        ...

    @abstractmethod
    def build_part(
        self,
        now: datetime,
        for_part: str,
        resume_state: Optional[S],
    ) -&gt; StatefulSourcePartition[X, S]:
        &#34;&#34;&#34;Build anew or resume an input partition.

        Will be called once per execution for each partition key on a
        worker that reported that partition was local in `list_parts`.

        Do not pre-build state about a partition in the
        constructor. All state must be derived from `resume_state` for
        recovery to work properly.

        Args:
            now: The current time.

            for_part: Which partition to build. Will always be one of
                the keys returned by `list_parts` on this worker.

            resume_state: State data containing where in the input
                stream this partition should be begin reading during
                this execution.

        Returns:
            The built partition.

        &#34;&#34;&#34;
        ...


class StatelessSourcePartition(ABC, Generic[X]):
    &#34;&#34;&#34;Input partition that is stateless.&#34;&#34;&#34;

    @abstractmethod
    def next_batch(self, sched: datetime) -&gt; Iterable[X]:
        &#34;&#34;&#34;Attempt to get the next batch of input items.

        This must participate in a kind of cooperative multi-tasking,
        never blocking but yielding an empty list if there are no new
        items yet.

        Args:
            sched: The scheduled awake time.

        Returns:
            Items immediately ready. May be empty if no new items.

        Raises:
            StopIteration: When the source is complete.

        &#34;&#34;&#34;
        ...

    def next_awake(self) -&gt; Optional[datetime]:
        &#34;&#34;&#34;When to next attempt to get input items.

        `next_batch()` will not be called until the most recently returned
        time has past.

        This will be called upon initialization of the source and
        after `next_batch()`, but also possibly at other times. Multiple
        times are not stored; you must return the next awake time on
        every call, if any.

        If this returns `None`, `next_batch()` will be called
        immediately unless the previous batch had no items, in which
        case there is a 1 millisecond delay.

        Use this instead of `time.sleep` in `next_batch()`.

        Returns:
            Next awake time or `None` to indicate automatic behavior.

        &#34;&#34;&#34;
        return None

    def close(self) -&gt; None:
        &#34;&#34;&#34;Cleanup this partition when the dataflow completes.

        This is not guaranteed to be called. It will only be called
        when the dataflow finishes on finite input. It will not be
        called during an abrupt or abort shutdown.

        &#34;&#34;&#34;
        return


class DynamicSource(Source[X]):
    &#34;&#34;&#34;An input source where all workers can read distinct items.

    Does not support storing any resume state. Thus these kind of
    sources only naively can support at-most-once processing.

    The source must somehow support supplying disjoint data for each
    worker. If you re-read the same items on multiple workers, the
    dataflow will process these as duplicate items.

    &#34;&#34;&#34;

    @abstractmethod
    def build(
        self, now: datetime, worker_index: int, worker_count: int
    ) -&gt; StatelessSourcePartition[X]:
        &#34;&#34;&#34;Build an input source for a worker.

        Will be called once on each worker.

        Args:
            now: The current time.

            worker_index: Index of this worker.

            worker_count: Total number of workers.

        Returns:
            The built partition.

        &#34;&#34;&#34;
        ...


class _SimplePollingPartition(StatefulSourcePartition[X, None]):
    def __init__(
        self,
        now: datetime,
        interval: timedelta,
        align_to: Optional[datetime],
        getter: Callable[[], X],
    ):
        self._interval = interval
        self._getter = getter

        if align_to is not None:
            # Hell yeah timedelta implements remainder.
            since_last_awake = (now - align_to) % interval
            if since_last_awake &gt; timedelta(seconds=0):
                until_next_awake = interval - since_last_awake
            else:
                # If now is exactly on the align_to mark (remainder is
                # 0), don&#39;t wait a whole interval; activate
                # immediately.
                until_next_awake = timedelta(seconds=0)
            self._next_awake = now + until_next_awake
        else:
            self._next_awake = now

    def next_batch(self, _sched: datetime) -&gt; List[X]:
        try:
            item = self._getter()
            self._next_awake += self._interval
            if item is None:
                return []
            return [item]
        except SimplePollingSource.Retry as ex:
            self._next_awake += ex.timeout
            return []

    def next_awake(self) -&gt; Optional[datetime]:
        return self._next_awake

    def snapshot(self) -&gt; None:
        return None


class SimplePollingSource(FixedPartitionedSource[X, None]):
    &#34;&#34;&#34;Calls a user defined function at a regular interval.

    &gt;&gt;&gt; class URLSource(SimplePollingSource):
    ...     def __init__(self):
    ...         super(interval=timedelta(seconds=10))
    ...
    ...     def next_item(self):
    ...         res = requests.get(&#34;https://example.com&#34;)
    ...         if not res.ok:
    ...             raise SimplePollingSource.Retry(timedelta(seconds=1))
    ...         return res.text

    There is no parallelism; only one worker will poll this source.

    Does not support storing any resume state. Thus these kind of
    sources only naively can support at-most-once processing.

    This is best for low-throughput polling on the order of seconds to
    hours.

    If you need a high-throughput source, or custom retry or timing,
    avoid this. Instead create a source using one of the other
    `Source` subclasses where you can have increased paralellism,
    batching, and finer control over timing.

    &#34;&#34;&#34;

    @dataclass
    class Retry(Exception):
        &#34;&#34;&#34;Raise this to try to get items before the usual interval.

        Args:
            timeout: How long to wait before calling
                `SimplePollingSource.next_item` again.

        &#34;&#34;&#34;

        timeout: timedelta

    def __init__(self, interval: timedelta, align_to: Optional[datetime] = None):
        &#34;&#34;&#34;Init.

        Args:
            interval:
                The interval between calling `next_item`.
            align_to:
                Align awake times to the given datetime. Defaults to
                now.

        &#34;&#34;&#34;
        self._interval = interval
        self._align_to = align_to

    def list_parts(self) -&gt; List[str]:
        &#34;&#34;&#34;Assumes the source has a single partition.&#34;&#34;&#34;
        return [&#34;singleton&#34;]

    def build_part(
        self, now: datetime, _for_part: str, _resume_state: Optional[None]
    ) -&gt; _SimplePollingPartition[X]:
        &#34;&#34;&#34;See ABC docstring.&#34;&#34;&#34;
        return _SimplePollingPartition(
            now, self._interval, self._align_to, self.next_item
        )

    @abstractmethod
    def next_item(self) -&gt; X:
        &#34;&#34;&#34;Override with custom logic to poll your source.

        Raises:
            Retry: Raise if you can&#39;t fetch items and would like to
                call this function sooner than the usual interval.

        Returns:
            Next item to emit into the dataflow. If `None`, no item is
            emitted.

        &#34;&#34;&#34;
        ...


def batch(ib: Iterable[X], batch_size: int) -&gt; Iterator[List[X]]:
    &#34;&#34;&#34;Batch an iterable.

    Use this to easily generate batches of items for a source&#39;s
    `next_batch` method.

    Args:
        ib:
            The underlying source iterable of items.
        batch_size:
            Maximum number of items to yield in a batch.

    Yields:
        The next gathered batch of items.

    &#34;&#34;&#34;
    # Ensure that we have the stateful iterator of the source.
    it = iter(ib)
    while True:
        batch = list(islice(it, batch_size))
        if len(batch) &lt;= 0:
            return
        yield batch


def batch_getter(
    getter: Callable[[], X], batch_size: int, yield_on: Optional[X] = None
) -&gt; Iterator[List[X]]:
    &#34;&#34;&#34;Batch from a getter function that might not return an item.

     Use this to easily generate batches of items for a source&#39;s
    `next_batch` method.

    Args:
        getter:
            Function to call to get the next item. Should raise
            `StopIteration` on EOF.

        batch_size:
            Maximum number of items to yield in a batch.

        yield_on:
            Sentinel value that indicates that there are no more items
            yet, and to return the current batch. Defaults to `None`.

    Yields:
        The next gathered batch of items.

    &#34;&#34;&#34;
    while True:
        batch: List[Any] = []
        while len(batch) &lt; batch_size:
            try:
                item = getter()
                if item != yield_on:
                    batch.append(item)
                else:
                    break
            except StopIteration:
                yield batch
                return
        yield batch


def batch_getter_ex(
    getter: Callable[[], X], batch_size: int, yield_ex: Type[Exception] = queue.Empty
) -&gt; Iterator[List[X]]:
    &#34;&#34;&#34;Batch from a getter function that raises on no items yet.

     Use this to easily generate batches of items for a source&#39;s
    `next_batch` method.

    Args:
        getter:
            Function to call to get the next item. Should raise
            `StopIteration` on EOF.

        batch_size:
            Maximum number of items to return in a batch.

        yield_ex:
            Exception raised by `getter` that indicates that there are
            no more items yet, and to return the current
            batch. Defaults to `queue.Empty`.

    Yields:
        The next gathered batch of items.

    &#34;&#34;&#34;
    while True:
        batch: List[Any] = []
        while len(batch) &lt; batch_size:
            try:
                item = getter()
                batch.append(item)
            except yield_ex:
                break
            except StopIteration:
                yield batch
                return
        yield batch


def batch_async(
    aib: AsyncIterable,
    timeout: timedelta,
    batch_size: int,
    loop=None,
) -&gt; Iterator[List[Any]]:
    &#34;&#34;&#34;Batch an async iterable synchronously up to a timeout.

    This allows using an async iterator as an input source. The
    `next_batch` method on an input source must never block, this
    allows running an async iterator up to a timeout so that you
    correctly cooperatively multitask with the rest of the dataflow.

    Args:
        aib:
            The underlying source async iterable of items.
        timeout:
            Duration of time to repeatedly poll the source
            async iterator for items.
        batch_size:
            Maximum number of items to yield in a batch, even if
            the timeout has not been hit.
        loop:
            Custom `asyncio` run loop to use, if any.

    Yields:
        The next gathered batch of items.

        This function will take up to `timeout` time to yield, or
        will return a list with length up to `max_len`.

    &#34;&#34;&#34;
    # Ensure that we have the stateful iterator of the source.
    ait = aib.__aiter__()

    loop = loop if loop is not None else asyncio.new_event_loop()
    task = None

    async def anext_batch():
        nonlocal task

        batch = []
        # Only try to gather this many items.
        for _ in range(batch_size):
            if task is None:
                task = loop.create_task(ait.__anext__())

            try:
                # Prevent the `wait_for` cancellation from
                # stopping the `__anext__` task; usually all
                # sub-tasks are cancelled too. It&#39;ll be re-used in
                # the next batch.
                next_item = await asyncio.shield(task)
            except asyncio.CancelledError:
                # Timeout was hit and thus return the batch
                # immediately.
                break
            except StopAsyncIteration:
                if len(batch) &gt; 0:
                    # Return a half-finished batch if we run out
                    # of source items.
                    break
                else:
                    # We can&#39;t raise `StopIteration` directly here
                    # because it&#39;s part of the coro protocol and
                    # would mess with this async function.
                    raise

            batch.append(next_item)
            task = None
        return batch

    while True:
        try:
            # `wait_for` will raise `CancelledError` at the internal
            # await point in `anext_batch` if the timeout is hit.
            batch = loop.run_until_complete(
                asyncio.wait_for(anext_batch(), timeout.total_seconds())
            )
            yield batch
        except StopAsyncIteration:
            return</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="api__article-subtitle" id="header-functions">Functions</h2>
<dl>
<dt id="bytewax.inputs.batch"><code class="language-python name flex">
<span>def <span class="ident">batch</span></span>(<span>ib: Iterable[~X], batch_size: int) ‑> Iterator[List[~X]]</span>
</code></dt>
<dd>
<div class="desc"><p>Batch an iterable.</p>
<p>Use this to easily generate batches of items for a source's
<code>next_batch</code> method.</p>
<h2 id="args">Args</h2>
<p>ib:
The underlying source iterable of items.
batch_size:
Maximum number of items to yield in a batch.</p>
<h2 id="yields">Yields</h2>
<p>The next gathered batch of items.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def batch(ib: Iterable[X], batch_size: int) -&gt; Iterator[List[X]]:
    &#34;&#34;&#34;Batch an iterable.

    Use this to easily generate batches of items for a source&#39;s
    `next_batch` method.

    Args:
        ib:
            The underlying source iterable of items.
        batch_size:
            Maximum number of items to yield in a batch.

    Yields:
        The next gathered batch of items.

    &#34;&#34;&#34;
    # Ensure that we have the stateful iterator of the source.
    it = iter(ib)
    while True:
        batch = list(islice(it, batch_size))
        if len(batch) &lt;= 0:
            return
        yield batch</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.batch_async"><code class="language-python name flex">
<span>def <span class="ident">batch_async</span></span>(<span>aib: collections.abc.AsyncIterable, timeout: datetime.timedelta, batch_size: int, loop=None) ‑> Iterator[List[Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>Batch an async iterable synchronously up to a timeout.</p>
<p>This allows using an async iterator as an input source. The
<code>next_batch</code> method on an input source must never block, this
allows running an async iterator up to a timeout so that you
correctly cooperatively multitask with the rest of the dataflow.</p>
<h2 id="args">Args</h2>
<p>aib:
The underlying source async iterable of items.
timeout:
Duration of time to repeatedly poll the source
async iterator for items.
batch_size:
Maximum number of items to yield in a batch, even if
the timeout has not been hit.
loop:
Custom <code>asyncio</code> run loop to use, if any.</p>
<h2 id="yields">Yields</h2>
<p>The next gathered batch of items.</p>
<p>This function will take up to <code>timeout</code> time to yield, or
will return a list with length up to <code>max_len</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def batch_async(
    aib: AsyncIterable,
    timeout: timedelta,
    batch_size: int,
    loop=None,
) -&gt; Iterator[List[Any]]:
    &#34;&#34;&#34;Batch an async iterable synchronously up to a timeout.

    This allows using an async iterator as an input source. The
    `next_batch` method on an input source must never block, this
    allows running an async iterator up to a timeout so that you
    correctly cooperatively multitask with the rest of the dataflow.

    Args:
        aib:
            The underlying source async iterable of items.
        timeout:
            Duration of time to repeatedly poll the source
            async iterator for items.
        batch_size:
            Maximum number of items to yield in a batch, even if
            the timeout has not been hit.
        loop:
            Custom `asyncio` run loop to use, if any.

    Yields:
        The next gathered batch of items.

        This function will take up to `timeout` time to yield, or
        will return a list with length up to `max_len`.

    &#34;&#34;&#34;
    # Ensure that we have the stateful iterator of the source.
    ait = aib.__aiter__()

    loop = loop if loop is not None else asyncio.new_event_loop()
    task = None

    async def anext_batch():
        nonlocal task

        batch = []
        # Only try to gather this many items.
        for _ in range(batch_size):
            if task is None:
                task = loop.create_task(ait.__anext__())

            try:
                # Prevent the `wait_for` cancellation from
                # stopping the `__anext__` task; usually all
                # sub-tasks are cancelled too. It&#39;ll be re-used in
                # the next batch.
                next_item = await asyncio.shield(task)
            except asyncio.CancelledError:
                # Timeout was hit and thus return the batch
                # immediately.
                break
            except StopAsyncIteration:
                if len(batch) &gt; 0:
                    # Return a half-finished batch if we run out
                    # of source items.
                    break
                else:
                    # We can&#39;t raise `StopIteration` directly here
                    # because it&#39;s part of the coro protocol and
                    # would mess with this async function.
                    raise

            batch.append(next_item)
            task = None
        return batch

    while True:
        try:
            # `wait_for` will raise `CancelledError` at the internal
            # await point in `anext_batch` if the timeout is hit.
            batch = loop.run_until_complete(
                asyncio.wait_for(anext_batch(), timeout.total_seconds())
            )
            yield batch
        except StopAsyncIteration:
            return</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.batch_getter"><code class="language-python name flex">
<span>def <span class="ident">batch_getter</span></span>(<span>getter: Callable[[], ~X], batch_size: int, yield_on: Optional[~X] = None) ‑> Iterator[List[~X]]</span>
</code></dt>
<dd>
<div class="desc"><p>Batch from a getter function that might not return an item.</p>
<p>Use this to easily generate batches of items for a source's
<code>next_batch</code> method.</p>
<h2 id="args">Args</h2>
<p>getter:
Function to call to get the next item. Should raise
<code>StopIteration</code> on EOF.</p>
<p>batch_size:
Maximum number of items to yield in a batch.</p>
<p>yield_on:
Sentinel value that indicates that there are no more items
yet, and to return the current batch. Defaults to <code>None</code>.</p>
<h2 id="yields">Yields</h2>
<p>The next gathered batch of items.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def batch_getter(
    getter: Callable[[], X], batch_size: int, yield_on: Optional[X] = None
) -&gt; Iterator[List[X]]:
    &#34;&#34;&#34;Batch from a getter function that might not return an item.

     Use this to easily generate batches of items for a source&#39;s
    `next_batch` method.

    Args:
        getter:
            Function to call to get the next item. Should raise
            `StopIteration` on EOF.

        batch_size:
            Maximum number of items to yield in a batch.

        yield_on:
            Sentinel value that indicates that there are no more items
            yet, and to return the current batch. Defaults to `None`.

    Yields:
        The next gathered batch of items.

    &#34;&#34;&#34;
    while True:
        batch: List[Any] = []
        while len(batch) &lt; batch_size:
            try:
                item = getter()
                if item != yield_on:
                    batch.append(item)
                else:
                    break
            except StopIteration:
                yield batch
                return
        yield batch</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.batch_getter_ex"><code class="language-python name flex">
<span>def <span class="ident">batch_getter_ex</span></span>(<span>getter: Callable[[], ~X], batch_size: int, yield_ex: Type[Exception] = _queue.Empty) ‑> Iterator[List[~X]]</span>
</code></dt>
<dd>
<div class="desc"><p>Batch from a getter function that raises on no items yet.</p>
<p>Use this to easily generate batches of items for a source's
<code>next_batch</code> method.</p>
<h2 id="args">Args</h2>
<p>getter:
Function to call to get the next item. Should raise
<code>StopIteration</code> on EOF.</p>
<p>batch_size:
Maximum number of items to return in a batch.</p>
<p>yield_ex:
Exception raised by <code>getter</code> that indicates that there are
no more items yet, and to return the current
batch. Defaults to <code>queue.Empty</code>.</p>
<h2 id="yields">Yields</h2>
<p>The next gathered batch of items.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def batch_getter_ex(
    getter: Callable[[], X], batch_size: int, yield_ex: Type[Exception] = queue.Empty
) -&gt; Iterator[List[X]]:
    &#34;&#34;&#34;Batch from a getter function that raises on no items yet.

     Use this to easily generate batches of items for a source&#39;s
    `next_batch` method.

    Args:
        getter:
            Function to call to get the next item. Should raise
            `StopIteration` on EOF.

        batch_size:
            Maximum number of items to return in a batch.

        yield_ex:
            Exception raised by `getter` that indicates that there are
            no more items yet, and to return the current
            batch. Defaults to `queue.Empty`.

    Yields:
        The next gathered batch of items.

    &#34;&#34;&#34;
    while True:
        batch: List[Any] = []
        while len(batch) &lt; batch_size:
            try:
                item = getter()
                batch.append(item)
            except yield_ex:
                break
            except StopIteration:
                yield batch
                return
        yield batch</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="api__article-subtitle" id="header-classes">Classes</h2>
<dl>
<dt id="bytewax.inputs.AbortExecution"><code class="language-python flex name class">
<span>class <span class="ident">AbortExecution</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Raise this from <code>next_batch</code> to abort for testing purposes.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="bytewax.inputs.DynamicSource"><code class="language-python flex name class">
<span>class <span class="ident">DynamicSource</span></span>
</code></dt>
<dd>
<div class="desc"><p>An input source where all workers can read distinct items.</p>
<p>Does not support storing any resume state. Thus these kind of
sources only naively can support at-most-once processing.</p>
<p>The source must somehow support supplying disjoint data for each
worker. If you re-read the same items on multiple workers, the
dataflow will process these as duplicate items.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class DynamicSource(Source[X]):
    &#34;&#34;&#34;An input source where all workers can read distinct items.

    Does not support storing any resume state. Thus these kind of
    sources only naively can support at-most-once processing.

    The source must somehow support supplying disjoint data for each
    worker. If you re-read the same items on multiple workers, the
    dataflow will process these as duplicate items.

    &#34;&#34;&#34;

    @abstractmethod
    def build(
        self, now: datetime, worker_index: int, worker_count: int
    ) -&gt; StatelessSourcePartition[X]:
        &#34;&#34;&#34;Build an input source for a worker.

        Will be called once on each worker.

        Args:
            now: The current time.

            worker_index: Index of this worker.

            worker_count: Total number of workers.

        Returns:
            The built partition.

        &#34;&#34;&#34;
        ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.inputs.Source" href="/apidocs/bytewax.inputs#bytewax.inputs.Source">Source</a></li>
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bytewax.inputs.DynamicSource.build"><code class="language-python name flex">
<span>def <span class="ident">build</span></span>(<span>self, now: datetime.datetime, worker_index: int, worker_count: int) ‑> <a title="bytewax.inputs.StatelessSourcePartition" href="/apidocs/bytewax.inputs#bytewax.inputs.StatelessSourcePartition">StatelessSourcePartition</a>[~X]</span>
</code></dt>
<dd>
<div class="desc"><p>Build an input source for a worker.</p>
<p>Will be called once on each worker.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>now</code></strong></dt>
<dd>The current time.</dd>
<dt><strong><code>worker_index</code></strong></dt>
<dd>Index of this worker.</dd>
<dt><strong><code>worker_count</code></strong></dt>
<dd>Total number of workers.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The built partition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def build(
    self, now: datetime, worker_index: int, worker_count: int
) -&gt; StatelessSourcePartition[X]:
    &#34;&#34;&#34;Build an input source for a worker.

    Will be called once on each worker.

    Args:
        now: The current time.

        worker_index: Index of this worker.

        worker_count: Total number of workers.

    Returns:
        The built partition.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bytewax.inputs.FixedPartitionedSource"><code class="language-python flex name class">
<span>class <span class="ident">FixedPartitionedSource</span></span>
</code></dt>
<dd>
<div class="desc"><p>An input source with a fixed number of independent partitions.</p>
<p>Will maintain the state of each source and re-build using it
during resume. If the source supports seeking, this input can
support exactly-once processing.</p>
<p>Each partition must contain unique data. If you re-read the same data
in multiple partitions, the dataflow will process these duplicate
items.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class FixedPartitionedSource(Source[X], Generic[X, S]):
    &#34;&#34;&#34;An input source with a fixed number of independent partitions.

    Will maintain the state of each source and re-build using it
    during resume. If the source supports seeking, this input can
    support exactly-once processing.

    Each partition must contain unique data. If you re-read the same data
    in multiple partitions, the dataflow will process these duplicate
    items.

    &#34;&#34;&#34;

    @abstractmethod
    def list_parts(self) -&gt; List[str]:
        &#34;&#34;&#34;List all local partitions this worker has access to.

        You do not need to list all partitions globally.

        Returns:
            Local partition keys.

        &#34;&#34;&#34;
        ...

    @abstractmethod
    def build_part(
        self,
        now: datetime,
        for_part: str,
        resume_state: Optional[S],
    ) -&gt; StatefulSourcePartition[X, S]:
        &#34;&#34;&#34;Build anew or resume an input partition.

        Will be called once per execution for each partition key on a
        worker that reported that partition was local in `list_parts`.

        Do not pre-build state about a partition in the
        constructor. All state must be derived from `resume_state` for
        recovery to work properly.

        Args:
            now: The current time.

            for_part: Which partition to build. Will always be one of
                the keys returned by `list_parts` on this worker.

            resume_state: State data containing where in the input
                stream this partition should be begin reading during
                this execution.

        Returns:
            The built partition.

        &#34;&#34;&#34;
        ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.inputs.Source" href="/apidocs/bytewax.inputs#bytewax.inputs.Source">Source</a></li>
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="bytewax.connectors.demo.RandomMetricSource" href="/apidocs/bytewax.connectors/demo#bytewax.connectors.demo.RandomMetricSource">RandomMetricSource</a></li>
<li><a title="bytewax.connectors.files.CSVSource" href="/apidocs/bytewax.connectors/files#bytewax.connectors.files.CSVSource">CSVSource</a></li>
<li><a title="bytewax.connectors.files.DirSource" href="/apidocs/bytewax.connectors/files#bytewax.connectors.files.DirSource">DirSource</a></li>
<li><a title="bytewax.connectors.files.FileSource" href="/apidocs/bytewax.connectors/files#bytewax.connectors.files.FileSource">FileSource</a></li>
<li><a title="bytewax.connectors.kafka.KafkaSource" href="/apidocs/bytewax.connectors/kafka#bytewax.connectors.kafka.KafkaSource">KafkaSource</a></li>
<li><a title="bytewax.inputs.SimplePollingSource" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource">SimplePollingSource</a></li>
<li><a title="bytewax.testing.TestingSource" href="/apidocs/bytewax.testing#bytewax.testing.TestingSource">TestingSource</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bytewax.inputs.FixedPartitionedSource.build_part"><code class="language-python name flex">
<span>def <span class="ident">build_part</span></span>(<span>self, now: datetime.datetime, for_part: str, resume_state: Optional[~S]) ‑> <a title="bytewax.inputs.StatefulSourcePartition" href="/apidocs/bytewax.inputs#bytewax.inputs.StatefulSourcePartition">StatefulSourcePartition</a>[~X, ~S]</span>
</code></dt>
<dd>
<div class="desc"><p>Build anew or resume an input partition.</p>
<p>Will be called once per execution for each partition key on a
worker that reported that partition was local in <code>list_parts</code>.</p>
<p>Do not pre-build state about a partition in the
constructor. All state must be derived from <code>resume_state</code> for
recovery to work properly.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>now</code></strong></dt>
<dd>The current time.</dd>
<dt><strong><code>for_part</code></strong></dt>
<dd>Which partition to build. Will always be one of
the keys returned by <code>list_parts</code> on this worker.</dd>
<dt><strong><code>resume_state</code></strong></dt>
<dd>State data containing where in the input
stream this partition should be begin reading during
this execution.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The built partition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def build_part(
    self,
    now: datetime,
    for_part: str,
    resume_state: Optional[S],
) -&gt; StatefulSourcePartition[X, S]:
    &#34;&#34;&#34;Build anew or resume an input partition.

    Will be called once per execution for each partition key on a
    worker that reported that partition was local in `list_parts`.

    Do not pre-build state about a partition in the
    constructor. All state must be derived from `resume_state` for
    recovery to work properly.

    Args:
        now: The current time.

        for_part: Which partition to build. Will always be one of
            the keys returned by `list_parts` on this worker.

        resume_state: State data containing where in the input
            stream this partition should be begin reading during
            this execution.

    Returns:
        The built partition.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.FixedPartitionedSource.list_parts"><code class="language-python name flex">
<span>def <span class="ident">list_parts</span></span>(<span>self) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>List all local partitions this worker has access to.</p>
<p>You do not need to list all partitions globally.</p>
<h2 id="returns">Returns</h2>
<p>Local partition keys.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def list_parts(self) -&gt; List[str]:
    &#34;&#34;&#34;List all local partitions this worker has access to.

    You do not need to list all partitions globally.

    Returns:
        Local partition keys.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bytewax.inputs.SimplePollingSource"><code class="language-python flex name class">
<span>class <span class="ident">SimplePollingSource</span></span>
<span>(</span><span>interval: datetime.timedelta, align_to: Optional[datetime.datetime] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls a user defined function at a regular interval.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class URLSource(SimplePollingSource):
...     def __init__(self):
...         super(interval=timedelta(seconds=10))
...
...     def next_item(self):
...         res = requests.get(&quot;https://example.com&quot;)
...         if not res.ok:
...             raise SimplePollingSource.Retry(timedelta(seconds=1))
...         return res.text
</code></pre>
<p>There is no parallelism; only one worker will poll this source.</p>
<p>Does not support storing any resume state. Thus these kind of
sources only naively can support at-most-once processing.</p>
<p>This is best for low-throughput polling on the order of seconds to
hours.</p>
<p>If you need a high-throughput source, or custom retry or timing,
avoid this. Instead create a source using one of the other
<code><a title="bytewax.inputs.Source" href="/apidocs/bytewax.inputs#bytewax.inputs.Source">Source</a></code> subclasses where you can have increased paralellism,
batching, and finer control over timing.</p>
<p>Init.</p>
<h2 id="args">Args</h2>
<p>interval:
The interval between calling <code>next_item</code>.
align_to:
Align awake times to the given datetime. Defaults to
now.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class SimplePollingSource(FixedPartitionedSource[X, None]):
    &#34;&#34;&#34;Calls a user defined function at a regular interval.

    &gt;&gt;&gt; class URLSource(SimplePollingSource):
    ...     def __init__(self):
    ...         super(interval=timedelta(seconds=10))
    ...
    ...     def next_item(self):
    ...         res = requests.get(&#34;https://example.com&#34;)
    ...         if not res.ok:
    ...             raise SimplePollingSource.Retry(timedelta(seconds=1))
    ...         return res.text

    There is no parallelism; only one worker will poll this source.

    Does not support storing any resume state. Thus these kind of
    sources only naively can support at-most-once processing.

    This is best for low-throughput polling on the order of seconds to
    hours.

    If you need a high-throughput source, or custom retry or timing,
    avoid this. Instead create a source using one of the other
    `Source` subclasses where you can have increased paralellism,
    batching, and finer control over timing.

    &#34;&#34;&#34;

    @dataclass
    class Retry(Exception):
        &#34;&#34;&#34;Raise this to try to get items before the usual interval.

        Args:
            timeout: How long to wait before calling
                `SimplePollingSource.next_item` again.

        &#34;&#34;&#34;

        timeout: timedelta

    def __init__(self, interval: timedelta, align_to: Optional[datetime] = None):
        &#34;&#34;&#34;Init.

        Args:
            interval:
                The interval between calling `next_item`.
            align_to:
                Align awake times to the given datetime. Defaults to
                now.

        &#34;&#34;&#34;
        self._interval = interval
        self._align_to = align_to

    def list_parts(self) -&gt; List[str]:
        &#34;&#34;&#34;Assumes the source has a single partition.&#34;&#34;&#34;
        return [&#34;singleton&#34;]

    def build_part(
        self, now: datetime, _for_part: str, _resume_state: Optional[None]
    ) -&gt; _SimplePollingPartition[X]:
        &#34;&#34;&#34;See ABC docstring.&#34;&#34;&#34;
        return _SimplePollingPartition(
            now, self._interval, self._align_to, self.next_item
        )

    @abstractmethod
    def next_item(self) -&gt; X:
        &#34;&#34;&#34;Override with custom logic to poll your source.

        Raises:
            Retry: Raise if you can&#39;t fetch items and would like to
                call this function sooner than the usual interval.

        Returns:
            Next item to emit into the dataflow. If `None`, no item is
            emitted.

        &#34;&#34;&#34;
        ...</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="bytewax.inputs.FixedPartitionedSource" href="/apidocs/bytewax.inputs#bytewax.inputs.FixedPartitionedSource">FixedPartitionedSource</a></li>
<li><a title="bytewax.inputs.Source" href="/apidocs/bytewax.inputs#bytewax.inputs.Source">Source</a></li>
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bytewax.inputs.SimplePollingSource.Retry"><code class="language-python name">var <span class="ident">Retry</span></code></dt>
<dd>
<div class="desc"><p>Raise this to try to get items before the usual interval.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>timeout</code></strong></dt>
<dd>How long to wait before calling
<code><a title="bytewax.inputs.SimplePollingSource.next_item" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource.next_item">SimplePollingSource.next_item()</a></code> again.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bytewax.inputs.SimplePollingSource.build_part"><code class="language-python name flex">
<span>def <span class="ident">build_part</span></span>(<span>self, now: datetime.datetime, _for_part: str, _resume_state: None) ‑> bytewax.inputs._SimplePollingPartition[~X]</span>
</code></dt>
<dd>
<div class="desc"><p>See ABC docstring.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def build_part(
    self, now: datetime, _for_part: str, _resume_state: Optional[None]
) -&gt; _SimplePollingPartition[X]:
    &#34;&#34;&#34;See ABC docstring.&#34;&#34;&#34;
    return _SimplePollingPartition(
        now, self._interval, self._align_to, self.next_item
    )</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.SimplePollingSource.list_parts"><code class="language-python name flex">
<span>def <span class="ident">list_parts</span></span>(<span>self) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Assumes the source has a single partition.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def list_parts(self) -&gt; List[str]:
    &#34;&#34;&#34;Assumes the source has a single partition.&#34;&#34;&#34;
    return [&#34;singleton&#34;]</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.SimplePollingSource.next_item"><code class="language-python name flex">
<span>def <span class="ident">next_item</span></span>(<span>self) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Override with custom logic to poll your source.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Retry</code></dt>
<dd>Raise if you can't fetch items and would like to
call this function sooner than the usual interval.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Next item to emit into the dataflow. If <code>None</code>, no item is
emitted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def next_item(self) -&gt; X:
    &#34;&#34;&#34;Override with custom logic to poll your source.

    Raises:
        Retry: Raise if you can&#39;t fetch items and would like to
            call this function sooner than the usual interval.

    Returns:
        Next item to emit into the dataflow. If `None`, no item is
        emitted.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bytewax.inputs.Source"><code class="language-python flex name class">
<span>class <span class="ident">Source</span></span>
</code></dt>
<dd>
<div class="desc"><p>A location to read input items from.</p>
<p>Base class for all input sources. Do not subclass this.</p>
<p>If you want to implement a custom connector, instead subclass one
of the specific source sub-types below in this module.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class Source(ABC, Generic[X]):  # noqa: B024
    &#34;&#34;&#34;A location to read input items from.

    Base class for all input sources. Do not subclass this.

    If you want to implement a custom connector, instead subclass one
    of the specific source sub-types below in this module.

    &#34;&#34;&#34;

    pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="bytewax.inputs.DynamicSource" href="/apidocs/bytewax.inputs#bytewax.inputs.DynamicSource">DynamicSource</a></li>
<li><a title="bytewax.inputs.FixedPartitionedSource" href="/apidocs/bytewax.inputs#bytewax.inputs.FixedPartitionedSource">FixedPartitionedSource</a></li>
</ul>
</dd>
<dt id="bytewax.inputs.StatefulSourcePartition"><code class="language-python flex name class">
<span>class <span class="ident">StatefulSourcePartition</span></span>
</code></dt>
<dd>
<div class="desc"><p>Input partition that maintains state of its position.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class StatefulSourcePartition(ABC, Generic[X, S]):
    &#34;&#34;&#34;Input partition that maintains state of its position.&#34;&#34;&#34;

    @abstractmethod
    def next_batch(self, sched: datetime) -&gt; Iterable[X]:
        &#34;&#34;&#34;Attempt to get the next batch of input items.

        This must participate in a kind of cooperative multi-tasking,
        never blocking but returning an empty list if there are no
        items to emit yet.

        Args:
            sched: The scheduled awake time.

        Returns:
            Items immediately ready. May be empty if no new items.

        Raises:
            StopIteration: When the source is complete.

        &#34;&#34;&#34;
        ...

    def next_awake(self) -&gt; Optional[datetime]:
        &#34;&#34;&#34;When to next attempt to get input items.

        `next_batch()` will not be called until the most recently returned
        time has past.

        This will be called upon initialization of the source and
        after `next_batch()`, but also possibly at other times. Multiple
        times are not stored; you must return the next awake time on
        every call, if any.

        If this returns `None`, `next_batch()` will be called
        immediately unless the previous batch had no items, in which
        case there is a 1 millisecond delay.

        Use this instead of `time.sleep` in `next_batch()`.

        Returns:
            Next awake time or `None` to indicate automatic behavior.

        &#34;&#34;&#34;
        return None

    @abstractmethod
    def snapshot(self) -&gt; S:
        &#34;&#34;&#34;Snapshot the position of the next read of this partition.

        This will be returned to you via the `resume_state` parameter
        of your input builder.

        Be careful of &#34;off by one&#34; errors in resume state. This should
        return a state that, when built into a partition, resumes reading
        _after the last read item item_, not the same item that
        `next()` last returned.

        This is guaranteed to never be called after `close()`.

        Returns:
            Resume state.

        &#34;&#34;&#34;
        ...

    def close(self) -&gt; None:
        &#34;&#34;&#34;Cleanup this partition when the dataflow completes.

        This is not guaranteed to be called. It will only be called
        when the dataflow finishes on finite input. It will not be
        called during an abrupt or abort shutdown.

        &#34;&#34;&#34;
        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>bytewax.connectors.demo._RandomMetricPartition</li>
<li>bytewax.connectors.files._CSVPartition</li>
<li>bytewax.connectors.files._FileSourcePartition</li>
<li>bytewax.connectors.kafka._KafkaSourcePartition</li>
<li>bytewax.inputs._SimplePollingPartition</li>
<li>bytewax.testing._IterSourcePartition</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bytewax.inputs.StatefulSourcePartition.close"><code class="language-python name flex">
<span>def <span class="ident">close</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Cleanup this partition when the dataflow completes.</p>
<p>This is not guaranteed to be called. It will only be called
when the dataflow finishes on finite input. It will not be
called during an abrupt or abort shutdown.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def close(self) -&gt; None:
    &#34;&#34;&#34;Cleanup this partition when the dataflow completes.

    This is not guaranteed to be called. It will only be called
    when the dataflow finishes on finite input. It will not be
    called during an abrupt or abort shutdown.

    &#34;&#34;&#34;
    return</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.StatefulSourcePartition.next_awake"><code class="language-python name flex">
<span>def <span class="ident">next_awake</span></span>(<span>self) ‑> Optional[datetime.datetime]</span>
</code></dt>
<dd>
<div class="desc"><p>When to next attempt to get input items.</p>
<p><code>next_batch()</code> will not be called until the most recently returned
time has past.</p>
<p>This will be called upon initialization of the source and
after <code>next_batch()</code>, but also possibly at other times. Multiple
times are not stored; you must return the next awake time on
every call, if any.</p>
<p>If this returns <code>None</code>, <code>next_batch()</code> will be called
immediately unless the previous batch had no items, in which
case there is a 1 millisecond delay.</p>
<p>Use this instead of <code>time.sleep</code> in <code>next_batch()</code>.</p>
<h2 id="returns">Returns</h2>
<p>Next awake time or <code>None</code> to indicate automatic behavior.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def next_awake(self) -&gt; Optional[datetime]:
    &#34;&#34;&#34;When to next attempt to get input items.

    `next_batch()` will not be called until the most recently returned
    time has past.

    This will be called upon initialization of the source and
    after `next_batch()`, but also possibly at other times. Multiple
    times are not stored; you must return the next awake time on
    every call, if any.

    If this returns `None`, `next_batch()` will be called
    immediately unless the previous batch had no items, in which
    case there is a 1 millisecond delay.

    Use this instead of `time.sleep` in `next_batch()`.

    Returns:
        Next awake time or `None` to indicate automatic behavior.

    &#34;&#34;&#34;
    return None</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.StatefulSourcePartition.next_batch"><code class="language-python name flex">
<span>def <span class="ident">next_batch</span></span>(<span>self, sched: datetime.datetime) ‑> Iterable[~X]</span>
</code></dt>
<dd>
<div class="desc"><p>Attempt to get the next batch of input items.</p>
<p>This must participate in a kind of cooperative multi-tasking,
never blocking but returning an empty list if there are no
items to emit yet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sched</code></strong></dt>
<dd>The scheduled awake time.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Items immediately ready. May be empty if no new items.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>StopIteration</code></dt>
<dd>When the source is complete.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def next_batch(self, sched: datetime) -&gt; Iterable[X]:
    &#34;&#34;&#34;Attempt to get the next batch of input items.

    This must participate in a kind of cooperative multi-tasking,
    never blocking but returning an empty list if there are no
    items to emit yet.

    Args:
        sched: The scheduled awake time.

    Returns:
        Items immediately ready. May be empty if no new items.

    Raises:
        StopIteration: When the source is complete.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.StatefulSourcePartition.snapshot"><code class="language-python name flex">
<span>def <span class="ident">snapshot</span></span>(<span>self) ‑> ~S</span>
</code></dt>
<dd>
<div class="desc"><p>Snapshot the position of the next read of this partition.</p>
<p>This will be returned to you via the <code>resume_state</code> parameter
of your input builder.</p>
<p>Be careful of "off by one" errors in resume state. This should
return a state that, when built into a partition, resumes reading
<em>after the last read item item</em>, not the same item that
<code>next()</code> last returned.</p>
<p>This is guaranteed to never be called after <code>close()</code>.</p>
<h2 id="returns">Returns</h2>
<p>Resume state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def snapshot(self) -&gt; S:
    &#34;&#34;&#34;Snapshot the position of the next read of this partition.

    This will be returned to you via the `resume_state` parameter
    of your input builder.

    Be careful of &#34;off by one&#34; errors in resume state. This should
    return a state that, when built into a partition, resumes reading
    _after the last read item item_, not the same item that
    `next()` last returned.

    This is guaranteed to never be called after `close()`.

    Returns:
        Resume state.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bytewax.inputs.StatelessSourcePartition"><code class="language-python flex name class">
<span>class <span class="ident">StatelessSourcePartition</span></span>
</code></dt>
<dd>
<div class="desc"><p>Input partition that is stateless.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">class StatelessSourcePartition(ABC, Generic[X]):
    &#34;&#34;&#34;Input partition that is stateless.&#34;&#34;&#34;

    @abstractmethod
    def next_batch(self, sched: datetime) -&gt; Iterable[X]:
        &#34;&#34;&#34;Attempt to get the next batch of input items.

        This must participate in a kind of cooperative multi-tasking,
        never blocking but yielding an empty list if there are no new
        items yet.

        Args:
            sched: The scheduled awake time.

        Returns:
            Items immediately ready. May be empty if no new items.

        Raises:
            StopIteration: When the source is complete.

        &#34;&#34;&#34;
        ...

    def next_awake(self) -&gt; Optional[datetime]:
        &#34;&#34;&#34;When to next attempt to get input items.

        `next_batch()` will not be called until the most recently returned
        time has past.

        This will be called upon initialization of the source and
        after `next_batch()`, but also possibly at other times. Multiple
        times are not stored; you must return the next awake time on
        every call, if any.

        If this returns `None`, `next_batch()` will be called
        immediately unless the previous batch had no items, in which
        case there is a 1 millisecond delay.

        Use this instead of `time.sleep` in `next_batch()`.

        Returns:
            Next awake time or `None` to indicate automatic behavior.

        &#34;&#34;&#34;
        return None

    def close(self) -&gt; None:
        &#34;&#34;&#34;Cleanup this partition when the dataflow completes.

        This is not guaranteed to be called. It will only be called
        when the dataflow finishes on finite input. It will not be
        called during an abrupt or abort shutdown.

        &#34;&#34;&#34;
        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="bytewax.inputs.StatelessSourcePartition.close"><code class="language-python name flex">
<span>def <span class="ident">close</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Cleanup this partition when the dataflow completes.</p>
<p>This is not guaranteed to be called. It will only be called
when the dataflow finishes on finite input. It will not be
called during an abrupt or abort shutdown.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def close(self) -&gt; None:
    &#34;&#34;&#34;Cleanup this partition when the dataflow completes.

    This is not guaranteed to be called. It will only be called
    when the dataflow finishes on finite input. It will not be
    called during an abrupt or abort shutdown.

    &#34;&#34;&#34;
    return</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.StatelessSourcePartition.next_awake"><code class="language-python name flex">
<span>def <span class="ident">next_awake</span></span>(<span>self) ‑> Optional[datetime.datetime]</span>
</code></dt>
<dd>
<div class="desc"><p>When to next attempt to get input items.</p>
<p><code>next_batch()</code> will not be called until the most recently returned
time has past.</p>
<p>This will be called upon initialization of the source and
after <code>next_batch()</code>, but also possibly at other times. Multiple
times are not stored; you must return the next awake time on
every call, if any.</p>
<p>If this returns <code>None</code>, <code>next_batch()</code> will be called
immediately unless the previous batch had no items, in which
case there is a 1 millisecond delay.</p>
<p>Use this instead of <code>time.sleep</code> in <code>next_batch()</code>.</p>
<h2 id="returns">Returns</h2>
<p>Next awake time or <code>None</code> to indicate automatic behavior.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">def next_awake(self) -&gt; Optional[datetime]:
    &#34;&#34;&#34;When to next attempt to get input items.

    `next_batch()` will not be called until the most recently returned
    time has past.

    This will be called upon initialization of the source and
    after `next_batch()`, but also possibly at other times. Multiple
    times are not stored; you must return the next awake time on
    every call, if any.

    If this returns `None`, `next_batch()` will be called
    immediately unless the previous batch had no items, in which
    case there is a 1 millisecond delay.

    Use this instead of `time.sleep` in `next_batch()`.

    Returns:
        Next awake time or `None` to indicate automatic behavior.

    &#34;&#34;&#34;
    return None</code></pre>
</details>
</dd>
<dt id="bytewax.inputs.StatelessSourcePartition.next_batch"><code class="language-python name flex">
<span>def <span class="ident">next_batch</span></span>(<span>self, sched: datetime.datetime) ‑> Iterable[~X]</span>
</code></dt>
<dd>
<div class="desc"><p>Attempt to get the next batch of input items.</p>
<p>This must participate in a kind of cooperative multi-tasking,
never blocking but yielding an empty list if there are no new
items yet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sched</code></strong></dt>
<dd>The scheduled awake time.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Items immediately ready. May be empty if no new items.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>StopIteration</code></dt>
<dd>When the source is complete.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre class="language-python line-numbers"><code class="language-python">@abstractmethod
def next_batch(self, sched: datetime) -&gt; Iterable[X]:
    &#34;&#34;&#34;Attempt to get the next batch of input items.

    This must participate in a kind of cooperative multi-tasking,
    never blocking but yielding an empty list if there are no new
    items yet.

    Args:
        sched: The scheduled awake time.

    Returns:
        Items immediately ready. May be empty if no new items.

    Raises:
        StopIteration: When the source is complete.

    &#34;&#34;&#34;
    ...</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
<footer class="api__footer" id="footer">
<p class="api__footer-copyright">
Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.
</p>
</footer>
</article>
<nav class="api__sidebar" id="sidebar">
<ul class="api__sidebar-nav" id="index">
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title">Super-module</h3>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item">
<a title="bytewax" href="/apidocs/">bytewax</a>
</li>
</ul>
</li>
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title"><a href="#header-functions">Functions</a></h3>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.batch" href="/apidocs/bytewax.inputs#bytewax.inputs.batch">batch</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.batch_async" href="/apidocs/bytewax.inputs#bytewax.inputs.batch_async">batch_async</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.batch_getter" href="/apidocs/bytewax.inputs#bytewax.inputs.batch_getter">batch_getter</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.batch_getter_ex" href="/apidocs/bytewax.inputs#bytewax.inputs.batch_getter_ex">batch_getter_ex</a></li>
</ul>
</li>
<li class="api__sidebar-nav-item">
<h3 class="api__sidebar-nav-title"><a href="#header-classes">Classes</a></h3>
<ul class="api__sidebar-nav-classes">
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.AbortExecution" href="/apidocs/bytewax.inputs#bytewax.inputs.AbortExecution">AbortExecution</a></h4>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.DynamicSource" href="/apidocs/bytewax.inputs#bytewax.inputs.DynamicSource">DynamicSource</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.DynamicSource.build" href="/apidocs/bytewax.inputs#bytewax.inputs.DynamicSource.build">build</a></li>
</ul>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.FixedPartitionedSource" href="/apidocs/bytewax.inputs#bytewax.inputs.FixedPartitionedSource">FixedPartitionedSource</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.FixedPartitionedSource.build_part" href="/apidocs/bytewax.inputs#bytewax.inputs.FixedPartitionedSource.build_part">build_part</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.FixedPartitionedSource.list_parts" href="/apidocs/bytewax.inputs#bytewax.inputs.FixedPartitionedSource.list_parts">list_parts</a></li>
</ul>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.SimplePollingSource" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource">SimplePollingSource</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.SimplePollingSource.Retry" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource.Retry">Retry</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.SimplePollingSource.build_part" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource.build_part">build_part</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.SimplePollingSource.list_parts" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource.list_parts">list_parts</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.SimplePollingSource.next_item" href="/apidocs/bytewax.inputs#bytewax.inputs.SimplePollingSource.next_item">next_item</a></li>
</ul>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.Source" href="/apidocs/bytewax.inputs#bytewax.inputs.Source">Source</a></h4>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.StatefulSourcePartition" href="/apidocs/bytewax.inputs#bytewax.inputs.StatefulSourcePartition">StatefulSourcePartition</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatefulSourcePartition.close" href="/apidocs/bytewax.inputs#bytewax.inputs.StatefulSourcePartition.close">close</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatefulSourcePartition.next_awake" href="/apidocs/bytewax.inputs#bytewax.inputs.StatefulSourcePartition.next_awake">next_awake</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatefulSourcePartition.next_batch" href="/apidocs/bytewax.inputs#bytewax.inputs.StatefulSourcePartition.next_batch">next_batch</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatefulSourcePartition.snapshot" href="/apidocs/bytewax.inputs#bytewax.inputs.StatefulSourcePartition.snapshot">snapshot</a></li>
</ul>
</li>
<li class="api__sidebar-nav-classes-item">
<h4 class="api__sidebar-nav-classes-title"><a title="bytewax.inputs.StatelessSourcePartition" href="/apidocs/bytewax.inputs#bytewax.inputs.StatelessSourcePartition">StatelessSourcePartition</a></h4>
<ul class="api__sidebar-nav-menu">
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatelessSourcePartition.close" href="/apidocs/bytewax.inputs#bytewax.inputs.StatelessSourcePartition.close">close</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatelessSourcePartition.next_awake" href="/apidocs/bytewax.inputs#bytewax.inputs.StatelessSourcePartition.next_awake">next_awake</a></li>
<li class="api__sidebar-nav-menu-item"><a title="bytewax.inputs.StatelessSourcePartition.next_batch" href="/apidocs/bytewax.inputs#bytewax.inputs.StatelessSourcePartition.next_batch">next_batch</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
